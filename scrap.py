"""
PROJETO PORTF√ìLIO: ANALISADOR DE LIVROS - WEB SCRAPING √âTICO
Autor: Ot√°vio Augusto De Souza Tavares
Data: 24/09/2025
Descri√ß√£o: Projeto educacional de web scraping usando sites de demonstra√ß√£o
"""

import requests
from bs4 import BeautifulSoup
import pandas as pd
import time
import json
import os
from datetime import datetime
import logging

# Configurar logging para monitoramento
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(levelname)s - %(message)s',
    handlers=[
        logging.FileHandler('scraping_log.log'),
        logging.StreamHandler()
    ]
)

class AnalisadorLivros:
    """
    Classe para an√°lise √©tica de livros usando web scraping
    Sites utilizados: Books to Scrape (site de demonstra√ß√£o)
    """
    
    def __init__(self):
        self.session = requests.Session()
        self.setup_headers()
        self.dados_coletados = []
        
    def setup_headers(self):
        """Configurar headers √©ticos para identificar o bot"""
        self.session.headers.update({
            'User-Agent': 'AnalisadorLivros-Educacional/1.0 (+https://github.com/seu-usuario/portfolio-web-scraping)',
            'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8',
            'Accept-Language': 'pt-BR,pt;q=0.9,en;q=0.8',
            'Accept-Encoding': 'gzip, deflate',
            'Connection': 'keep-alive',
        })
    
    def verificar_robots_txt(self, base_url):
        """
        Verificar robots.txt antes de fazer scraping
        Boa pr√°tica √©tica importante
        """
        try:
            robots_url = f"{base_url}/robots.txt"
            response = self.session.get(robots_url, timeout=10)
            
            if response.status_code == 200:
                logging.info("üìã Robots.txt encontrado:")
                logging.info(f"Conte√∫do do robots.txt:\n{response.text[:500]}...")
                
                # Verificar se scraping √© permitido
                if 'Disallow: /' in response.text:
                    logging.warning("‚ö†Ô∏è  Robots.txt pode restringir acesso total")
                else:
                    logging.info("‚úÖ Robots.txt permite acesso")
                    
            return True
        except Exception as e:
            logging.warning(f"‚ùå N√£o foi poss√≠vel verificar robots.txt: {e}")
            return False
    
    def fazer_requisicao_segura(self, url, delay=1):
        """
        Fazer requisi√ß√£o HTTP com tratamento de erros e delays
        """
        try:
            time.sleep(delay)  # Respeitar o servidor
            
            response = self.session.get(url, timeout=15)
            response.raise_for_status()  # Levanta exce√ß√£o para status 4xx/5xx
            
            logging.info(f"‚úÖ Requisi√ß√£o bem-sucedida: {url} (Status: {response.status_code})")
            return response
            
        except requests.exceptions.Timeout:
            logging.error(f"‚è∞ Timeout na requisi√ß√£o: {url}")
            return None
        except requests.exceptions.HTTPError as e:
            logging.error(f"üö´ Erro HTTP {response.status_code}: {url} - {e}")
            return None
        except requests.exceptions.RequestException as e:
            logging.error(f"üîå Erro de conex√£o: {url} - {e}")
            return None
    
    def extrair_info_livro(self, livro_element):
        """
        Extrair informa√ß√µes de um elemento livro individual
        """
        try:
            # Extrair t√≠tulo
            titulo = livro_element.h3.a['title']
            
            # Extrair pre√ßo
            preco_element = livro_element.find('p', class_='price_color')
            preco = preco_element.text if preco_element else 'N/A'
            
            # Extrair disponibilidade
            disponibilidade_element = livro_element.find('p', class_='instock availability')
            disponibilidade = disponibilidade_element.text.strip() if disponibilidade_element else 'N/A'
            
            # Extrair avalia√ß√£o (rating)
            rating_element = livro_element.find('p', class_='star-rating')
            rating_classes = rating_element.get('class', []) if rating_element else []
            rating = next((cls for cls in rating_classes if cls != 'star-rating'), 'N/A')
            
            # Converter rating para n√∫mero
            rating_map = {'One': 1, 'Two': 2, 'Three': 3, 'Four': 4, 'Five': 5}
            rating_numero = rating_map.get(rating, 0)
            
            return {
                'titulo': titulo,
                'preco': preco,
                'disponibilidade': disponibilidade,
                'rating_texto': rating,
                'rating_numero': rating_numero,
                'timestamp_coleta': datetime.now().isoformat()
            }
            
        except Exception as e:
            logging.error(f"‚ùå Erro ao extrair info do livro: {e}")
            return None
    
    def scrape_pagina_unica(self, url):
        """
        Fazer scraping de uma √∫nica p√°gina
        """
        logging.info(f"üîç Iniciando scraping da p√°gina: {url}")
        
        response = self.fazer_requisicao_segura(url)
        if not response:
            return []
        
        soup = BeautifulSoup(response.content, 'html.parser')
        
        # Encontrar todos os livros na p√°gina
        livros = soup.find_all('article', class_='product_pod')
        logging.info(f"üìö Encontrados {len(livros)} livros na p√°gina")
        
        dados_pagina = []
        
        for i, livro in enumerate(livros, 1):
            info_livro = self.extrair_info_livro(livro)
            if info_livro:
                info_livro['numero_sequencia'] = i
                dados_pagina.append(info_livro)
                logging.info(f"‚úÖ Livro {i} processado: {info_livro['titulo'][:30]}...")
        
        return dados_pagina
    
    def scrape_multiplas_paginas(self, url_base, total_paginas=3):
        """
        Fazer scraping de m√∫ltiplas p√°ginas com controle
        """
        logging.info(f"üöÄ Iniciando scraping de {total_paginas} p√°ginas")
        
        todas_info_livros = []
        
        for pagina in range(1, total_paginas + 1):
            url = f"{url_base}/catalogue/page-{pagina}.html"
            logging.info(f"üìÑ Processando p√°gina {pagina}/{total_paginas}")
            
            livros_pagina = self.scrape_pagina_unica(url)
            for livro in livros_pagina:
                livro['pagina'] = pagina
                todas_info_livros.append(livro)
            
            # Delay maior entre p√°ginas para ser √©tico
            time.sleep(2)
        
        logging.info(f"üéØ Total de livros coletados: {len(todas_info_livros)}")
        return todas_info_livros
    
    def analisar_dados(self, dados_livros):
        """
        Realizar an√°lise b√°sica dos dados coletados
        """
        if not dados_livros:
            logging.warning("üìä Nenhum dado para analisar")
            return {}
        
        df = pd.DataFrame(dados_livros)
        
        analise = {
            'total_livros': len(dados_livros),
            'preco_medio': 'N/A',
            'avaliacao_media': 0,
            'distribuicao_rating': {},
            'livros_por_pagina': {}
        }
        
        try:
            # An√°lise de pre√ßos (remover s√≠mbolo de moeda)
            precos = df['preco'].str.replace('¬£', '').astype(float)
            analise['preco_medio'] = f"¬£{precos.mean():.2f}"
            
            # An√°lise de ratings
            analise['avaliacao_media'] = df['rating_numero'].mean()
            analise['distribuicao_rating'] = df['rating_texto'].value_counts().to_dict()
            analise['livros_por_pagina'] = df['pagina'].value_counts().sort_index().to_dict()
            
        except Exception as e:
            logging.error(f"‚ùå Erro na an√°lise: {e}")
        
        return analise
    
    def exportar_dados(self, dados_livros, analise):
        """
        Exportar dados em m√∫ltiplos formatos para portf√≥lio
        """
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        
        # Criar diret√≥rio de resultados
        os.makedirs('resultados', exist_ok=True)
        
        # Exportar para CSV
        df = pd.DataFrame(dados_livros)
        csv_path = f'resultados/livros_analisados_{timestamp}.csv'
        df.to_csv(csv_path, index=False, encoding='utf-8')
        
        # Exportar para JSON
        json_path = f'resultados/livros_analisados_{timestamp}.json'
        with open(json_path, 'w', encoding='utf-8') as f:
            json.dump({
                'metadata': {
                    'data_coleta': timestamp,
                    'total_livros': len(dados_livros),
                    'projeto': 'Analisador de Livros - Web Scraping √âtico'
                },
                'analise': analise,
                'dados': dados_livros
            }, f, indent=2, ensure_ascii=False)
        
        # Exportar an√°lise resumida
        resumo_path = f'resultados/resumo_analise_{timestamp}.txt'
        with open(resumo_path, 'w', encoding='utf-8') as f:
            f.write("=== RESUMO DA AN√ÅLISE DE LIVROS ===\n\n")
            f.write(f"Data da coleta: {timestamp}\n")
            f.write(f"Total de livros analisados: {analise['total_livros']}\n")
            f.write(f"Pre√ßo m√©dio: {analise['preco_medio']}\n")
            f.write(f"Avalia√ß√£o m√©dia: {analise['avaliacao_media']:.2f}/5.0\n\n")
            
            f.write("Distribui√ß√£o por rating:\n")
            for rating, count in analise['distribuicao_rating'].items():
                f.write(f"  {rating}: {count} livros\n")
        
        logging.info(f"üíæ Dados exportados:")
        logging.info(f"   CSV: {csv_path}")
        logging.info(f"   JSON: {json_path}")
        logging.info(f"   Resumo: {resumo_path}")
        
        return csv_path, json_path
    
    def gerar_relatorio(self, analise):
        """
        Gerar relat√≥rio formatado para exibi√ß√£o
        """
        print("\n" + "="*60)
        print("üìä RELAT√ìRIO DE AN√ÅLISE - WEB SCRAPING √âTICO")
        print("="*60)
        
        print(f"üìö Total de livros analisados: {analise['total_livros']}")
        print(f"üí∞ Pre√ßo m√©dio: {analise['preco_medio']}")
        print(f"‚≠ê Avalia√ß√£o m√©dia: {analise['avaliacao_media']:.2f}/5.0")
        
        print("\nüìà Distribui√ß√£o por avalia√ß√£o:")
        for rating, count in analise.get('distribuicao_rating', {}).items():
            print(f"   {rating}: {count} livros")
        
        print("\nüåê Livros por p√°gina:")
        for pagina, count in analise.get('livros_por_pagina', {}).items():
            print(f"   P√°gina {pagina}: {count} livros")
        
        print("\n‚úÖ Coleta realizada com pr√°ticas √©ticas de web scraping")
        print("="*60)

def main():
    """
    Fun√ß√£o principal - executar todo o pipeline
    """
    print("üöÄ INICIANDO ANALISADOR DE LIVROS - WEB SCRAPING √âTICO")
    print("üìç Site utilizado: Books to Scrape (site de demonstra√ß√£o)")
    print("‚è∞ Aguarde, isso pode levar alguns segundos...\n")
    
    # Inicializar analisador
    analisador = AnalisadorLivros()
    
    # Configura√ß√µes
    BASE_URL = "http://books.toscrape.com"
    TOTAL_PAGINAS = 3  # Limitar para demonstra√ß√£o
    
    try:
        # 1. Verificar robots.txt (boas pr√°ticas)
        analisador.verificar_robots_txt(BASE_URL)
        
        # 2. Fazer scraping das p√°ginas
        dados_livros = analisador.scrape_multiplas_paginas(BASE_URL, TOTAL_PAGINAS)
        
        if not dados_livros:
            logging.error("‚ùå Nenhum dado foi coletado")
            return
        
        # 3. Analisar dados
        analise = analisador.analisar_dados(dados_livros)
        
        # 4. Exportar dados
        analisador.exportar_dados(dados_livros, analise)
        
        # 5. Gerar relat√≥rio
        analisador.gerar_relatorio(analise)
        
        logging.info("üéâ Processo conclu√≠do com sucesso!")
        
    except KeyboardInterrupt:
        logging.info("‚èπÔ∏è  Processo interrompido pelo usu√°rio")
    except Exception as e:
        logging.error(f"üí• Erro cr√≠tico: {e}")

if __name__ == "__main__":
    main()